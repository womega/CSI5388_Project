{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T02:01:28.096475Z",
     "start_time": "2020-11-15T02:01:11.551971Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tldextract import extract\n",
    "import wordninja\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import dnstwist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:10:37.902149Z",
     "start_time": "2020-11-10T19:10:37.515070Z"
    }
   },
   "outputs": [],
   "source": [
    "unlabeled_data = pd.read_csv('final_unlabeled.csv')\n",
    "malicious_data = pd.read_csv('final_malicious.csv')\n",
    "whitelist_data = pd.read_csv('final_whitelist_with_keyword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:10:37.918143Z",
     "start_time": "2020-11-10T19:10:37.904153Z"
    }
   },
   "outputs": [],
   "source": [
    "unlabeled_list = unlabeled_data['Domain'].values.tolist()\n",
    "malicious_list = malicious_data['Domain'].values.tolist()\n",
    "whitelist_list = whitelist_data['Domain'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:10:38.103918Z",
     "start_time": "2020-11-10T19:10:37.920141Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def check_num_word(list):\n",
    "    lens = []\n",
    "    l = 0\n",
    "    for i in list:\n",
    "        l += len(wordninja.split(i))\n",
    "        lens.append(len(wordninja.split(i)))\n",
    "    print(\"The average number of words is: {:.2f}\".format(l/len(list)))\n",
    "    return lens\n",
    "\n",
    "def check_num_char(list):\n",
    "    lens = []\n",
    "    l = 0\n",
    "    for i in list:\n",
    "        le = 0\n",
    "        for j in extract(i):\n",
    "            le += len(j)\n",
    "        l += le\n",
    "        lens.append(le)\n",
    "    print(\"The average number of characters is: {:.2f}\".format(l/len(list)))\n",
    "    return lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:14:20.565146Z",
     "start_time": "2020-11-10T19:10:38.112881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of words is: 5.22\n",
      "The average number of words is: 7.70\n",
      "The average number of words is: 8.47\n"
     ]
    }
   ],
   "source": [
    "malicious_num_words = check_num_word(malicious_list)\n",
    "whitelist_num_words = check_num_word(whitelist_list)\n",
    "unlabeled_num_words = check_num_word(unlabeled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:14:25.876089Z",
     "start_time": "2020-11-10T19:14:20.567146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of characters is: 20.21\n",
      "The average number of characters is: 26.30\n",
      "The average number of characters is: 30.66\n"
     ]
    }
   ],
   "source": [
    "malicious_num_chars = check_num_char(malicious_list)\n",
    "whitelist_num_chars = check_num_char(whitelist_list)\n",
    "unlabeled_num_chars = check_num_char(unlabeled_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:14:25.892105Z",
     "start_time": "2020-11-10T19:14:25.879087Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_hyphen(list):\n",
    "    hyphen = []\n",
    "    for i in list:\n",
    "        if '-' in i:\n",
    "            hyphen.append(1)\n",
    "        else:\n",
    "            hyphen.append(0)\n",
    "    print(\"The number of domains containing hyphens is \"\n",
    "          \"{:}, making {:.2f}% of the data\".format(hyphen.count(1), (hyphen.count(1)*100/len(list))))\n",
    "    return hyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:14:26.097316Z",
     "start_time": "2020-11-10T19:14:25.895079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of domains containing hyphens is 16959, making 27.51% of the data\n",
      "The number of domains containing hyphens is 1135, making 19.01% of the data\n",
      "The number of domains containing hyphens is 154509, making 42.29% of the data\n"
     ]
    }
   ],
   "source": [
    "malicious_hyphen = check_hyphen(malicious_list)\n",
    "whitelist_hyphen = check_hyphen(whitelist_list)\n",
    "unlabeled_hyphen = check_hyphen(unlabeled_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:11:09.558986Z",
     "start_time": "2020-11-15T04:11:09.514036Z"
    },
    "code_folding": [
     0,
     4,
     7
    ]
   },
   "outputs": [],
   "source": [
    "def entropy(s):\n",
    "    p, lns = Counter(s), float(len(s))\n",
    "    return -sum(count/lns * math.log(count/lns,2) for count in p.values())\n",
    "\n",
    "def check_entropy(l):\n",
    "    entropies = []\n",
    "    e = 0\n",
    "    for i in l:\n",
    "        entropies.append(entropy(i))\n",
    "        e += entropy(i)\n",
    "    print(\"The average entropy is {:.2f}\".format(e/(len(l))))\n",
    "    return entropies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:11:10.140900Z",
     "start_time": "2020-11-15T04:11:10.070809Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_tld(array):\n",
    "    new_list = []\n",
    "    count = 0\n",
    "    for i in array:\n",
    "        tsd, td, tsu = extract(i) # subdomain, domain, suffix\n",
    "        if tsd:\n",
    "            if td:\n",
    "                dom = tsd + '.' + td + '.' + tsu\n",
    "            else:\n",
    "                dom = tsd + '.' + tsu\n",
    "        elif td:\n",
    "            dom = td + '.' + tsu\n",
    "        else:\n",
    "            dom = tsu\n",
    "        new_list.append(dom)\n",
    "    return(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:11:10.502232Z",
     "start_time": "2020-11-15T04:11:10.492222Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_td(array):\n",
    "    new_list = []\n",
    "    for i in array:\n",
    "        tsd, td, tsu = extract(i) #subdomain, domain, suffix\n",
    "        if td:\n",
    "            dom = td\n",
    "        else:\n",
    "            dom = tsu\n",
    "        new_list.append(dom)\n",
    "    return(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:11:10.788272Z",
     "start_time": "2020-11-15T04:11:10.750889Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_td1(array):\n",
    "    new_list = []\n",
    "    for i in array:\n",
    "        tsd, td, tsu = extract(i) #subdomain, domain, suffix\n",
    "        if td:\n",
    "            dom = td + '.' + tsu\n",
    "        else:\n",
    "            dom = tsu\n",
    "        new_list.append(dom)\n",
    "    return(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:11:11.103885Z",
     "start_time": "2020-11-15T04:11:11.090877Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_tld1(array):\n",
    "    new_list = []\n",
    "    count = 0\n",
    "    for i in array:\n",
    "        tsd, td, tsu = extract(i) # subdomain, domain, suffix\n",
    "        if tsd:\n",
    "            if td:\n",
    "                dom = tsd + '.' + td\n",
    "            else:\n",
    "                dom = tsd\n",
    "        elif td:\n",
    "            dom = td\n",
    "        else:\n",
    "            dom = tsu\n",
    "        new_list.append(dom)\n",
    "    return(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** entropy with subdomain and suffix **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:11:31.232519Z",
     "start_time": "2020-11-15T04:11:16.805714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average entropy is 3.66\n",
      "The average entropy is 3.75\n",
      "The average entropy is 3.79\n"
     ]
    }
   ],
   "source": [
    "malicious_entropy = check_entropy(malicious_list)\n",
    "whitelist_entropy = check_entropy(whitelist_list)\n",
    "unlabeled_entropy = check_entropy(unlabeled_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** entropy without suffix **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:11:51.368067Z",
     "start_time": "2020-11-15T04:11:32.526666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average entropy is 3.45\n",
      "The average entropy is 3.58\n",
      "The average entropy is 3.70\n"
     ]
    }
   ],
   "source": [
    "malicious_entropy1 = check_entropy(check_tld1(malicious_list))\n",
    "whitelist_entropy1 = check_entropy(check_tld1(whitelist_list))\n",
    "unlabeled_entropy1 = check_entropy(check_tld1(unlabeled_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** entropy without subdomain and suffix **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:12:06.122568Z",
     "start_time": "2020-11-15T04:11:51.370051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average entropy is 3.45\n",
      "The average entropy is 2.70\n",
      "The average entropy is 2.83\n"
     ]
    }
   ],
   "source": [
    "malicious_entropy_nosd = check_entropy(check_td(malicious_list))\n",
    "whitelist_entropy_nosd = check_entropy(check_td(whitelist_list))\n",
    "unlabeled_entropy_nosd = check_entropy(check_td(unlabeled_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranco Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:14:56.418897Z",
     "start_time": "2020-11-10T19:14:55.404994Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top1m = pd.read_csv('top-1m.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:14:56.434872Z",
     "start_time": "2020-11-10T19:14:56.420881Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# dropping the subdomain to check the rank of the domain\n",
    "def check_td_1(array):\n",
    "    new_list = []\n",
    "    for i in array:\n",
    "        tsd, td, tsu = extract(i) #subdomain, domain, suffix\n",
    "        if td:\n",
    "            dom = td + '.' + tsu\n",
    "        else:\n",
    "            dom = tsu\n",
    "        new_list.append(dom)\n",
    "    return(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T19:14:56.560232Z",
     "start_time": "2020-11-10T19:14:56.437887Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def check_tranco_rank(list):\n",
    "    ranks = []\n",
    "    for i in list:\n",
    "        if i in top1m[1].values:\n",
    "            ranks.append(1)\n",
    "        else:\n",
    "            ranks.append(0)\n",
    "    print(\"{:.2f}% of the domains are on the Tranco list.\".format((ranks.count(1)*100/len(list))))\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T21:46:32.053072Z",
     "start_time": "2020-11-10T19:14:56.564162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46% of the domains are on the Tranco list.\n",
      "40.76% of the domains are on the Tranco list.\n",
      "41.31% of the domains are on the Tranco list.\n"
     ]
    }
   ],
   "source": [
    "malicious_rank = check_tranco_rank(check_td1(malicious_list))\n",
    "whitelist_rank = check_tranco_rank(check_td1(whitelist_list))\n",
    "unlabeled_rank = check_tranco_rank(check_td1(unlabeled_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longest word Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T21:46:32.068063Z",
     "start_time": "2020-11-10T21:46:32.056071Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_ratio(list):\n",
    "    ratios = []\n",
    "    s = 0\n",
    "    for i in list:\n",
    "        original_len = len(i)\n",
    "        longest_element = max([len(x) for x in wordninja.split(i)])\n",
    "        ratio = longest_element/original_len\n",
    "        s += ratio\n",
    "        ratios.append(ratio)\n",
    "    print(\"The average longest word \"\n",
    "          \"ratio is {:.2f}\".format(s/(len(list))))\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T21:48:29.778875Z",
     "start_time": "2020-11-10T21:46:32.070062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average longest word ratio is 0.35\n",
      "The average longest word ratio is 0.25\n",
      "The average longest word ratio is 0.27\n"
     ]
    }
   ],
   "source": [
    "malicious_ratio = check_ratio(malicious_list)\n",
    "whitelist_ratio = check_ratio(whitelist_list)\n",
    "unlabeled_ratio = check_ratio(unlabeled_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typosquatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T03:25:56.654941Z",
     "start_time": "2020-11-15T03:25:56.637934Z"
    },
    "code_folding": [
     0,
     48
    ]
   },
   "outputs": [],
   "source": [
    "def generate_doms(keys):\n",
    "    fuzz_doms = []\n",
    "    fake_doms = []\n",
    "    # generate twisted domains from the provided keywords\n",
    "    \n",
    "    for i in keys:\n",
    "        i += '.com' # add a random suffix to pass through the fuzz generator\n",
    "        fuzz = dnstwist.DomainFuzz(i)\n",
    "        fuzz.generate()\n",
    "        fuzz_doms += fuzz.domains\n",
    "        \n",
    "    dic = {}\n",
    "    \n",
    "    for i in fuzz_doms:\n",
    "        key = i['fuzzer']\n",
    "        value = i['domain-name']\n",
    "        if key in dic:\n",
    "            if not isinstance(dic[key], list): \n",
    "                dic[key] = [dic[key]]\n",
    "            dic[key].append(value)\n",
    "        else:\n",
    "            dic[key] = value\n",
    "    \n",
    "    # drop the keys that are not relevent enough\n",
    "    # original keys: original*, addition, bitsquatting, homoglyph, hyphenation, insertion, \n",
    "    # repetition, replacement, transposition, various, vowel-swap\n",
    "    [dic.pop(x, None) for x in ['original*', 'subdomain', 'addition']]\n",
    "            \n",
    "    for key in dic.keys():\n",
    "        if isinstance(dic[key], list):\n",
    "            for l in dic[key]:\n",
    "                _, dom, _ = extract(l)\n",
    "                fake_doms.append(dom)\n",
    "        else:\n",
    "            _, dom, _ = extract(key)\n",
    "            fake_doms.append(dom)\n",
    "            \n",
    "                    \n",
    "        # fuzz_doms += fuzz.domains[1:] # drop the first result in list as it is the original keyword\n",
    "    for n in keys:\n",
    "        for k in fake_doms:\n",
    "            if k in n:\n",
    "                fake_doms.remove(k)\n",
    "    \n",
    "    #fake_doms.remove('covida') # portuguese of covid\n",
    "    \n",
    "    return fake_doms\n",
    "\n",
    "def check_typo(twists, array):\n",
    "    typo_doms = []\n",
    "    for i in array:\n",
    "        temp_mark = 0\n",
    "        for j in twists:\n",
    "            if (len(j) > 3) and (j in i):\n",
    "                temp_mark += 1 \n",
    "        #        print(i, j)\n",
    "        if temp_mark > 0:\n",
    "            typo_doms.append(1)\n",
    "        else:\n",
    "            typo_doms.append(0)\n",
    "    print(\"Detected {:.2f}% of domains with \"\n",
    "          \"typosquatting.\".format((typo_doms.count(1)*100/len(typo_doms))))\n",
    "    return typo_doms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T03:24:13.889370Z",
     "start_time": "2020-11-15T03:24:09.947413Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "keywords = ['corona', 'covid' , 'wuhan', 'ncov-19', 'coronavirus',\n",
    "            'virus', 'covid-19', 'covid19', 'wuhanvirus',\n",
    "            'novelvirus']\n",
    "\n",
    "fake_doms =  generate_doms(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T03:52:15.021555Z",
     "start_time": "2020-11-15T03:26:01.298382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 1.27% of domains with typosquatting.\n",
      "Detected 4.71% of domains with typosquatting.\n",
      "Detected 1.99% of domains with typosquatting.\n"
     ]
    }
   ],
   "source": [
    "whitelist_typo = check_typo(fake_doms, whitelist_list)\n",
    "malicious_typo = check_typo(fake_doms, malicious_list)\n",
    "unlabeled_typo = check_typo(fake_doms, unlabeled_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freenom TLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T21:48:29.902095Z",
     "start_time": "2020-11-10T21:48:29.796850Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def check_freenom(list):\n",
    "    freenoms = []\n",
    "    for i in list:\n",
    "        if i[-2:] == 'ml':\n",
    "            freenoms.append(1)\n",
    "        elif i[-2:] == 'cf':\n",
    "            freenoms.append(1)\n",
    "        elif i[-2:] == 'gq':\n",
    "            freenoms.append(1)\n",
    "        elif i[-2:] == 'tk':\n",
    "            freenoms.append(1)\n",
    "        elif i[-2:] == 'ga':\n",
    "            freenoms.append(1)\n",
    "        else:\n",
    "            freenoms.append(0)\n",
    "    print(\"{:.2f}% of domains have a \"\n",
    "          \"freenom TLD.\".format((freenoms.count(1)*100/len(freenoms))))\n",
    "    return freenoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T21:48:30.517671Z",
     "start_time": "2020-11-10T21:48:29.909030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.28% of domains have a freenom TLD.\n",
      "0.00% of domains have a freenom TLD.\n",
      "0.31% of domains have a freenom TLD.\n"
     ]
    }
   ],
   "source": [
    "malicious_freenom = check_freenom(malicious_list)\n",
    "whitelist_freenom = check_freenom(whitelist_list)\n",
    "unlabeled_freenom = check_freenom(unlabeled_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numbers other than 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T21:48:30.532676Z",
     "start_time": "2020-11-10T21:48:30.520667Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def check_num(list):\n",
    "    nums = []\n",
    "    for i in list:\n",
    "        num = re.findall(r'\\d', i)\n",
    "        if \"19\" in i:\n",
    "            if len(num) > 2:\n",
    "                nums.append(1)\n",
    "            else:\n",
    "                nums.append(0)\n",
    "        elif len(num):\n",
    "            nums.append(1)\n",
    "        else:\n",
    "            nums.append(0)\n",
    "    print(\"{:.2f}% of domains contain \"\n",
    "          \"numbers different than 19\".format((nums.count(1)*100/len(nums))))\n",
    "    return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T21:48:31.632208Z",
     "start_time": "2020-11-10T21:48:30.534659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.51% of domains contain numbers different than 19\n",
      "7.18% of domains contain numbers different than 19\n",
      "28.54% of domains contain numbers different than 19\n"
     ]
    }
   ],
   "source": [
    "malicious_num = check_num(malicious_list)\n",
    "whitelist_num = check_num(whitelist_list)\n",
    "unlabeled_num = check_num(unlabeled_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-08T08:32:35.977528Z",
     "start_time": "2020-11-08T08:15:25.937Z"
    }
   },
   "source": [
    "# Number of subdomains levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:15:49.803709Z",
     "start_time": "2020-11-15T04:15:49.747743Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def check_subdom(list):\n",
    "    subdoms = []\n",
    "    s = 0\n",
    "    for i in list:\n",
    "        subdom, _ , _ = extract(i)\n",
    "        if subdom:\n",
    "            subdoms.append(subdom.count('.') + 1)\n",
    "            s += subdom.count('.') + 1\n",
    "        else:\n",
    "            subdoms.append(0)\n",
    "    print(\"The average subdomain level is {:.2f}\".format(s/(len(list))))\n",
    "    return subdoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:15:55.276747Z",
     "start_time": "2020-11-15T04:15:49.876241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average subdomain level is 0.01\n",
      "The average subdomain level is 1.14\n",
      "The average subdomain level is 1.66\n"
     ]
    }
   ],
   "source": [
    "malicious_levels = check_subdom(malicious_list)\n",
    "whitelist_levels = check_subdom(whitelist_list)\n",
    "unlabeled_levels = check_subdom(unlabeled_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:33:33.689222Z",
     "start_time": "2020-11-15T04:33:33.629273Z"
    }
   },
   "outputs": [],
   "source": [
    "whitelist_label = np.zeros((len(whitelist_list),), dtype=int)\n",
    "malicious_label = np.ones((len(malicious_list),), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:33:57.262742Z",
     "start_time": "2020-11-15T04:33:57.159881Z"
    }
   },
   "outputs": [],
   "source": [
    "whitelist_dict = {\"Domain\":whitelist_list,\"Num_words\": whitelist_num_words,\n",
    "                  \"Num_chars\": whitelist_num_chars, \"Hyphen\": whitelist_hyphen,\n",
    "                  \"Entropy_sdsu\": whitelist_entropy, \"Entropy_nosdsu\": whitelist_entropy_nosd,\n",
    "                  \"Entropy_nosu\": whitelist_entropy1, \"Tranco_Rank\": whitelist_rank,\n",
    "                   \"Longest_word_ratio\": whitelist_ratio, \"Typos\": whitelist_typo,\n",
    "                   \"Freenom_TLD\": whitelist_freenom, \"Other_numbers\": whitelist_num,\n",
    "                   \"Subdomain levels\": whitelist_levels, \"Label\": whitelist_label}\n",
    "whitelist_df = pd.DataFrame(whitelist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:34:11.705251Z",
     "start_time": "2020-11-15T04:34:11.435611Z"
    }
   },
   "outputs": [],
   "source": [
    "malicious_dict = {\"Domain\":malicious_list,\"Num_words\": malicious_num_words,\n",
    "                  \"Num_chars\": malicious_num_chars, \"Hyphen\": malicious_hyphen,\n",
    "                  \"Entropy_sdsu\": malicious_entropy, \"Entropy_nosdsu\": malicious_entropy_nosd,\n",
    "                  \"Entropy_nosu\": malicious_entropy1, \"Tranco_Rank\": malicious_rank,\n",
    "                   \"Longest_word_ratio\": malicious_ratio, \"Typos\": malicious_typo,\n",
    "                   \"Freenom_TLD\": malicious_freenom, \"Other_numbers\": malicious_num,\n",
    "                   \"Subdomain levels\": malicious_levels, \"Label\": malicious_label}\n",
    "malicious_df = pd.DataFrame(malicious_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:18:47.695869Z",
     "start_time": "2020-11-15T04:18:46.085583Z"
    }
   },
   "outputs": [],
   "source": [
    "unlabeled_dict = {\"Domain\":unlabeled_list,\"Num_words\": unlabeled_num_words,\n",
    "                  \"Num_chars\": unlabeled_num_chars, \"Hyphen\": unlabeled_hyphen,\n",
    "                  \"Entropy_sdsu\": unlabeled_entropy, \"Entropy_nosdsu\": unlabeled_entropy_nosd,\n",
    "                  \"Entropy_nosu\": unlabeled_entropy1, \"Tranco_Rank\": unlabeled_rank,\n",
    "                   \"Longest_word_ratio\": unlabeled_ratio, \"Typos\": unlabeled_typo,\n",
    "                   \"Freenom_TLD\": unlabeled_freenom, \"Other_numbers\": unlabeled_num,\n",
    "                   \"Subdomain levels\": unlabeled_levels}\n",
    "unlabeled_df = pd.DataFrame(unlabeled_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:36:19.204968Z",
     "start_time": "2020-11-15T04:36:18.663620Z"
    }
   },
   "outputs": [],
   "source": [
    "final_dataset = pd.concat([whitelist_df, malicious_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:39:01.033279Z",
     "start_time": "2020-11-15T04:39:00.982292Z"
    }
   },
   "outputs": [],
   "source": [
    "final_dataset = final_dataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-15T04:39:29.325892Z",
     "start_time": "2020-11-15T04:39:22.226407Z"
    }
   },
   "outputs": [],
   "source": [
    "whitelist_df.to_csv('final_whitelist.csv', index=False,header=True)\n",
    "malicious_df.to_csv('final_malicious.csv', index=False,header=True)\n",
    "unlabeled_df.to_csv('final_unlabeled.csv', index=False,header=True)\n",
    "final_dataset.to_csv('final_dataset.csv', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
